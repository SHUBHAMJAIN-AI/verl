+ source /root/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/root/miniconda3/bin/conda
++ CONDA_EXE=/root/miniconda3/bin/conda
++ export _CONDA_EXE=/root/miniconda3/bin/conda
++ _CONDA_EXE=/root/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/root/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/root/miniconda3/bin/python
++ export _CONDA_ROOT=/root/miniconda3
++ _CONDA_ROOT=/root/miniconda3
++ '[' -z x ']'
+ conda activate verl_new
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate verl_new
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate verl_new
++ '[' -n '' ']'
++ /root/miniconda3/bin/conda shell.posix activate verl_new
+ ask_conda='export _CE_M='\'''\''
export _CE_CONDA='\'''\''
PS1='\''(verl_new) '\''
export PATH='\''/root/miniconda3/envs/verl_new/bin:/root/miniconda3/condabin:/root/.vscode-server/cli/servers/Stable-258e40fedc6cb8edf399a463ce3a9d32e7e1f6f3/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand'\''
export CONDA_SHLVL='\''2'\''
export CONDA_PROMPT_MODIFIER='\''(verl_new) '\''
export CONDA_EXE='\''/root/miniconda3/bin/conda'\''
export _CONDA_EXE='\''/root/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/root/miniconda3/bin/python'\''
export _CONDA_ROOT='\''/root/miniconda3'\'''
+ eval 'export _CE_M='\'''\''
export _CE_CONDA='\'''\''
PS1='\''(verl_new) '\''
export PATH='\''/root/miniconda3/envs/verl_new/bin:/root/miniconda3/condabin:/root/.vscode-server/cli/servers/Stable-258e40fedc6cb8edf399a463ce3a9d32e7e1f6f3/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand'\''
export CONDA_SHLVL='\''2'\''
export CONDA_PROMPT_MODIFIER='\''(verl_new) '\''
export CONDA_EXE='\''/root/miniconda3/bin/conda'\''
export _CONDA_EXE='\''/root/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/root/miniconda3/bin/python'\''
export _CONDA_ROOT='\''/root/miniconda3'\'''
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ PS1='(verl_new) '
++ export PATH=/root/miniconda3/envs/verl_new/bin:/root/miniconda3/condabin:/root/.vscode-server/cli/servers/Stable-258e40fedc6cb8edf399a463ce3a9d32e7e1f6f3/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand
++ PATH=/root/miniconda3/envs/verl_new/bin:/root/miniconda3/condabin:/root/.vscode-server/cli/servers/Stable-258e40fedc6cb8edf399a463ce3a9d32e7e1f6f3/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export 'CONDA_PROMPT_MODIFIER=(verl_new) '
++ CONDA_PROMPT_MODIFIER='(verl_new) '
++ export CONDA_EXE=/root/miniconda3/bin/conda
++ CONDA_EXE=/root/miniconda3/bin/conda
++ export _CONDA_EXE=/root/miniconda3/bin/conda
++ _CONDA_EXE=/root/miniconda3/bin/conda
++ export CONDA_PYTHON_EXE=/root/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/root/miniconda3/bin/python
++ export _CONDA_ROOT=/root/miniconda3
++ _CONDA_ROOT=/root/miniconda3
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ export CUDA_VISIBLE_DEVICES=0,1
+ CUDA_VISIBLE_DEVICES=0,1
+ export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
+ PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
+ tee verl_qwen0.5b_split_placement_run1.log
+ PYTHONUNBUFFERED=1
+ python3 main_ppo_split.py --config-path=/root/verl/examples/split_placement/config --config-name=qwen_0.5b_2gpu_split algorithm.adv_estimator=gae data.train_files=/root/verl/dataset/train_with_uid.parquet data.val_files=/root/verl/dataset/test_with_uid.parquet data.train_batch_size=512 data.max_prompt_length=512 data.max_response_length=512 data.filter_overlong_prompts=True data.truncation=error data.universal_id_key=uid data.reward_fn_key=data_source actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.actor.ppo_mini_batch_size=128 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 actor_rollout_ref.actor.ppo_max_token_len_per_gpu=4096 actor_rollout_ref.actor.fsdp_config.param_offload=True actor_rollout_ref.actor.fsdp_config.optimizer_offload=True +actor_rollout_ref.actor.loss_agg_mode=token-mean +actor_rollout_ref.actor.clip_ratio_low=0.2 +actor_rollout_ref.actor.clip_ratio_high=0.2 '+actor_rollout_ref.actor.checkpoint.contents=[model,optimizer,extra]' actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.max_num_batched_tokens=2048 actor_rollout_ref.rollout.max_num_seqs=256 +actor_rollout_ref.rollout.max_model_len=2048 +actor_rollout_ref.rollout.mode=sync actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 +actor_rollout_ref.rollout.val_kwargs.do_sample=false +actor_rollout_ref.rollout.val_kwargs.temperature=0 +actor_rollout_ref.rollout.val_kwargs.top_p=1.0 +actor_rollout_ref.rollout.val_kwargs.top_k=-1 +actor_rollout_ref.rollout.val_kwargs.n=1 +actor_rollout_ref.rollout.multi_turn.enable=false +critic.rollout_n=1 +critic.loss_agg_mode=token-mean '+critic.checkpoint.contents=[model,optimizer,extra]' critic.model.path=Qwen/Qwen2.5-0.5B-Instruct critic.optim.lr=1e-5 critic.ppo_micro_batch_size_per_gpu=16 critic.ppo_max_token_len_per_gpu=16384 critic.model.fsdp_config.param_offload=False critic.model.fsdp_config.optimizer_offload=False algorithm.use_kl_in_reward=False trainer.critic_warmup=0 'trainer.logger=[console,wandb]' trainer.project_name=verl_qwen_2gpu trainer.experiment_name=qwen_0.5b_split_test trainer.n_gpus_per_node=2 trainer.nnodes=1 trainer.total_epochs=10 trainer.save_freq=5
2025-09-07 20:47:24,722	INFO worker.py:1942 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(main_task pid=231343)[0m {'actor_rollout_ref': {'actor': {'checkpoint': {'contents': ['model',
[36m(main_task pid=231343)[0m                                                              'optimizer',
[36m(main_task pid=231343)[0m                                                              'extra']},
[36m(main_task pid=231343)[0m                                  'clip_ratio': 0.2,
[36m(main_task pid=231343)[0m                                  'clip_ratio_high': 0.2,
[36m(main_task pid=231343)[0m                                  'clip_ratio_low': 0.2,
[36m(main_task pid=231343)[0m                                  'entropy_coeff': 0.0,
[36m(main_task pid=231343)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=231343)[0m                                                  'optimizer_offload': True,
[36m(main_task pid=231343)[0m                                                  'param_offload': True,
[36m(main_task pid=231343)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=231343)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=231343)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=231343)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=231343)[0m                                  'loss_agg_mode': 'token-mean',
[36m(main_task pid=231343)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=231343)[0m                                            'lr_warmup_steps': -1,
[36m(main_task pid=231343)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=231343)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=231343)[0m                                            'total_training_steps': -1,
[36m(main_task pid=231343)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=231343)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=231343)[0m                                  'ppo_max_token_len_per_gpu': 4096,
[36m(main_task pid=231343)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=231343)[0m                                  'ppo_micro_batch_size_per_gpu': 8,
[36m(main_task pid=231343)[0m                                  'ppo_mini_batch_size': 128,
[36m(main_task pid=231343)[0m                                  'shuffle': False,
[36m(main_task pid=231343)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=231343)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=231343)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=231343)[0m                                  'use_kl_loss': False},
[36m(main_task pid=231343)[0m                        'hybrid_engine': True,
[36m(main_task pid=231343)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=231343)[0m                                  'external_lib': None,
[36m(main_task pid=231343)[0m                                  'override_config': {},
[36m(main_task pid=231343)[0m                                  'path': 'Qwen/Qwen2.5-0.5B-Instruct',
[36m(main_task pid=231343)[0m                                  'use_remove_padding': False},
[36m(main_task pid=231343)[0m                        'ref': {'fsdp_config': {'param_offload': False,
[36m(main_task pid=231343)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=231343)[0m                                'log_prob_max_token_len_per_gpu': 4096,
[36m(main_task pid=231343)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=231343)[0m                                'log_prob_micro_batch_size_per_gpu': None,
[36m(main_task pid=231343)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=231343)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=231343)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=231343)[0m                                    'do_sample': True,
[36m(main_task pid=231343)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=231343)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=231343)[0m                                    'enforce_eager': True,
[36m(main_task pid=231343)[0m                                    'free_cache_engine': True,
[36m(main_task pid=231343)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=231343)[0m                                    'ignore_eos': False,
[36m(main_task pid=231343)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=231343)[0m                                    'log_prob_max_token_len_per_gpu': 4096,
[36m(main_task pid=231343)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=231343)[0m                                    'log_prob_micro_batch_size_per_gpu': 8,
[36m(main_task pid=231343)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=231343)[0m                                    'max_model_len': 2048,
[36m(main_task pid=231343)[0m                                    'max_num_batched_tokens': 2048,
[36m(main_task pid=231343)[0m                                    'max_num_seqs': 256,
[36m(main_task pid=231343)[0m                                    'mode': 'sync',
[36m(main_task pid=231343)[0m                                    'multi_turn': {'enable': False},
[36m(main_task pid=231343)[0m                                    'n': 1,
[36m(main_task pid=231343)[0m                                    'name': 'vllm',
[36m(main_task pid=231343)[0m                                    'prompt_length': 512,
[36m(main_task pid=231343)[0m                                    'response_length': 512,
[36m(main_task pid=231343)[0m                                    'temperature': 1.0,
[36m(main_task pid=231343)[0m                                    'tensor_model_parallel_size': 1,
[36m(main_task pid=231343)[0m                                    'top_k': -1,
[36m(main_task pid=231343)[0m                                    'top_p': 1,
[36m(main_task pid=231343)[0m                                    'val_kwargs': {'do_sample': False,
[36m(main_task pid=231343)[0m                                                   'n': 1,
[36m(main_task pid=231343)[0m                                                   'temperature': 0,
[36m(main_task pid=231343)[0m                                                   'top_k': -1,
[36m(main_task pid=231343)[0m                                                   'top_p': 1.0}}},
[36m(main_task pid=231343)[0m  'algorithm': {'adv_estimator': 'gae',
[36m(main_task pid=231343)[0m                'gamma': 1.0,
[36m(main_task pid=231343)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=231343)[0m                'kl_penalty': 'kl',
[36m(main_task pid=231343)[0m                'lam': 1.0,
[36m(main_task pid=231343)[0m                'use_kl_in_reward': False},
[36m(main_task pid=231343)[0m  'critic': {'checkpoint': {'contents': ['model', 'optimizer', 'extra']},
[36m(main_task pid=231343)[0m             'cliprange_value': 0.5,
[36m(main_task pid=231343)[0m             'forward_max_token_len_per_gpu': 16384,
[36m(main_task pid=231343)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=231343)[0m             'forward_micro_batch_size_per_gpu': 16,
[36m(main_task pid=231343)[0m             'grad_clip': 1.0,
[36m(main_task pid=231343)[0m             'loss_agg_mode': 'token-mean',
[36m(main_task pid=231343)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=231343)[0m                       'external_lib': None,
[36m(main_task pid=231343)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=231343)[0m                                       'optimizer_offload': False,
[36m(main_task pid=231343)[0m                                       'param_offload': False,
[36m(main_task pid=231343)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=231343)[0m                       'override_config': {},
[36m(main_task pid=231343)[0m                       'path': 'Qwen/Qwen2.5-0.5B-Instruct',
[36m(main_task pid=231343)[0m                       'tokenizer_path': 'Qwen/Qwen2.5-0.5B-Instruct',
[36m(main_task pid=231343)[0m                       'use_remove_padding': False},
[36m(main_task pid=231343)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=231343)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=231343)[0m                       'min_lr_ratio': None,
[36m(main_task pid=231343)[0m                       'total_training_steps': -1,
[36m(main_task pid=231343)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=231343)[0m             'ppo_epochs': 1,
[36m(main_task pid=231343)[0m             'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=231343)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=231343)[0m             'ppo_micro_batch_size_per_gpu': 16,
[36m(main_task pid=231343)[0m             'ppo_mini_batch_size': 128,
[36m(main_task pid=231343)[0m             'rollout_n': 1,
[36m(main_task pid=231343)[0m             'shuffle': False,
[36m(main_task pid=231343)[0m             'strategy': 'fsdp',
[36m(main_task pid=231343)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=231343)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=231343)[0m  'data': {'filter_overlong_prompts': True,
[36m(main_task pid=231343)[0m           'max_prompt_length': 512,
[36m(main_task pid=231343)[0m           'max_response_length': 512,
[36m(main_task pid=231343)[0m           'prompt_key': 'prompt',
[36m(main_task pid=231343)[0m           'return_full_prompt': False,
[36m(main_task pid=231343)[0m           'return_raw_chat': False,
[36m(main_task pid=231343)[0m           'return_raw_input_ids': False,
[36m(main_task pid=231343)[0m           'reward_fn_key': 'data_source',
[36m(main_task pid=231343)[0m           'shuffle': True,
[36m(main_task pid=231343)[0m           'tokenizer': None,
[36m(main_task pid=231343)[0m           'train_batch_size': 512,
[36m(main_task pid=231343)[0m           'train_files': '/root/verl/dataset/train_with_uid.parquet',
[36m(main_task pid=231343)[0m           'truncation': 'error',
[36m(main_task pid=231343)[0m           'universal_id_key': 'uid',
[36m(main_task pid=231343)[0m           'val_batch_size': None,
[36m(main_task pid=231343)[0m           'val_files': '/root/verl/dataset/test_with_uid.parquet'},
[36m(main_task pid=231343)[0m  'ray_init': {'num_cpus': None},
[36m(main_task pid=231343)[0m  'reward_model': {'enable': False,
[36m(main_task pid=231343)[0m                   'forward_max_token_len_per_gpu': 16384,
[36m(main_task pid=231343)[0m                   'max_length': None,
[36m(main_task pid=231343)[0m                   'micro_batch_size': None,
[36m(main_task pid=231343)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=231343)[0m                   'model': {'external_lib': None,
[36m(main_task pid=231343)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=231343)[0m                                             'min_num_params': 0,
[36m(main_task pid=231343)[0m                                             'param_offload': False},
[36m(main_task pid=231343)[0m                             'input_tokenizer': 'Qwen/Qwen2.5-0.5B-Instruct',
[36m(main_task pid=231343)[0m                             'path': '~/models/reward_model',
[36m(main_task pid=231343)[0m                             'use_remove_padding': False},
[36m(main_task pid=231343)[0m                   'reward_manager': 'naive',
[36m(main_task pid=231343)[0m                   'strategy': 'fsdp',
[36m(main_task pid=231343)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=231343)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=231343)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=231343)[0m              'default_hdfs_dir': None,
[36m(main_task pid=231343)[0m              'default_local_dir': 'checkpoints/verl_qwen_2gpu/qwen_0.5b_split_test',
[36m(main_task pid=231343)[0m              'experiment_name': 'qwen_0.5b_split_test',
[36m(main_task pid=231343)[0m              'log_val_generations': 0,
[36m(main_task pid=231343)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=231343)[0m              'n_gpus_per_node': 2,
[36m(main_task pid=231343)[0m              'nnodes': 1,
[36m(main_task pid=231343)[0m              'project_name': 'verl_qwen_2gpu',
[36m(main_task pid=231343)[0m              'resume_from_path': None,
[36m(main_task pid=231343)[0m              'resume_mode': 'auto',
[36m(main_task pid=231343)[0m              'save_freq': 5,
[36m(main_task pid=231343)[0m              'test_freq': -1,
[36m(main_task pid=231343)[0m              'total_epochs': 10,
[36m(main_task pid=231343)[0m              'total_training_steps': None}}
[36m(main_task pid=231343)[0m resource_pool_spec: {'actor_rollout_ref_pool': [1], 'critic_pool': [1]}
[36m(main_task pid=231343)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=231343)[0m Using dataset class: RLHFDataset
[36m(main_task pid=231343)[0m Saving the dataset (0/1 shards):   0%|          | 0/7473 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 554163.36 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 548951.52 examples/s]
[36m(main_task pid=231343)[0m Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 360.63ba/s]
[36m(main_task pid=231343)[0m Saving the dataset (0/1 shards):   0%|          | 0/7473 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 538807.24 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 533933.53 examples/s]
[36m(main_task pid=231343)[0m dataset len: 7473
[36m(main_task pid=231343)[0m Sample uids from dataset:
[36m(main_task pid=231343)[0m daa002e4-b852-4a99-96de-b2746c4c9866 {'data_source': 'openai/gsm8k', 'prompt': '[{\'content\': \'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Let\\\'s think step by step and output the final answer after "####".\', \'role\': \'user\'}]', 'ability': 'math', 'reward_model': "{'ground_truth': '72', 'style': 'rule'}", 'extra_info': "{'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72', 'index': 0, 'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'split': 'train'}", 'uid': 'daa002e4-b852-4a99-96de-b2746c4c9866'}
[36m(main_task pid=231343)[0m 935e5ee2-09bb-4837-bd58-e92c4b075fd8 {'data_source': 'openai/gsm8k', 'prompt': '[{\'content\': \'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Let\\\'s think step by step and output the final answer after "####".\', \'role\': \'user\'}]', 'ability': 'math', 'reward_model': "{'ground_truth': '10', 'style': 'rule'}", 'extra_info': "{'answer': 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10', 'index': 1, 'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'split': 'train'}", 'uid': '935e5ee2-09bb-4837-bd58-e92c4b075fd8'}
[36m(main_task pid=231343)[0m 68878514-7b0c-4ccd-9558-556e0c3c43a0 {'data_source': 'openai/gsm8k', 'prompt': '[{\'content\': \'Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet? Let\\\'s think step by step and output the final answer after "####".\', \'role\': \'user\'}]', 'ability': 'math', 'reward_model': "{'ground_truth': '5', 'style': 'rule'}", 'extra_info': '{\'answer\': "In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\\nBetty\'s grandparents gave her 15 * 2 = $<<15*2=30>>30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\\n#### 5", \'index\': 2, \'question\': \'Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\', \'split\': \'train\'}', 'uid': '68878514-7b0c-4ccd-9558-556e0c3c43a0'}
[36m(main_task pid=231343)[0m Saving processed dataset with uid to dataset/processed_with_uid_hf
[36m(main_task pid=231343)[0m Saving processed dataset as parquet to dataset/processed_with_uid.parquet
[36m(main_task pid=231343)[0m Also saving to cache: /root/.cache/verl/rlhf/processed_with_uid
[36m(main_task pid=231343)[0m Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(main_task pid=231343)[0m WARNING:2025-09-07 20:47:31,026:Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):   0%|          | 0/7473 [00:00<?, ? examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  13%|█▎        | 935/7473 [00:02<00:16, 406.83 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  25%|██▌       | 1869/7473 [00:02<00:06, 927.86 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  50%|█████     | 3737/7473 [00:02<00:01, 2170.05 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  75%|███████▌  | 5605/7473 [00:02<00:00, 3588.80 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|██████████| 7473/7473 [00:02<00:00, 4661.45 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|██████████| 7473/7473 [00:03<00:00, 2432.09 examples/s]
[36m(main_task pid=231343)[0m Saving the dataset (0/1 shards):   0%|          | 0/7473 [00:00<?, ? examples/s]
[36m(main_task pid=231343)[0m filter dataset len: 7473
[36m(main_task pid=231343)[0m Saving filtered dataset to dataset/filtered_with_uid_hf
[36m(main_task pid=231343)[0m Saving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 93481.96 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 92766.21 examples/s]
[36m(main_task pid=231343)[0m Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]
[36m(main_task pid=231343)[0m Saving filtered dataset as parquet to dataset/filtered_with_uid.parquet
[36m(main_task pid=231343)[0m Creating parquet from Arrow format:  50%|█████     | 4/8 [00:00<00:00, 34.01ba/s]
[36m(main_task pid=231343)[0m Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 36.95ba/s]Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 36.43ba/s]
[36m(main_task pid=231343)[0m Saving the dataset (0/1 shards):   0%|          | 0/1319 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 239482.58 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 235096.34 examples/s]
[36m(main_task pid=231343)[0m Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 466.01ba/s]
[36m(main_task pid=231343)[0m Saving the dataset (0/1 shards):   0%|          | 0/1319 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 232722.82 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 227245.31 examples/s]
[36m(main_task pid=231343)[0m Using dataset class: RLHFDataset
[36m(main_task pid=231343)[0m dataset len: 1319
[36m(main_task pid=231343)[0m Sample uids from dataset:
[36m(main_task pid=231343)[0m 1f7c08ce-9ebe-4e16-a2e8-fbd39ad982f8 {'data_source': 'openai/gsm8k', 'prompt': '[{\'content\': \'Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers\\\' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers\\\' market? Let\\\'s think step by step and output the final answer after "####".\', \'role\': \'user\'}]', 'ability': 'math', 'reward_model': "{'ground_truth': '18', 'style': 'rule'}", 'extra_info': '{\'answer\': \'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18\', \'index\': 0, \'question\': "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers\' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers\' market?", \'split\': \'test\'}', 'uid': '1f7c08ce-9ebe-4e16-a2e8-fbd39ad982f8'}
[36m(main_task pid=231343)[0m 121ab1d2-b03c-4ea8-abda-1729f7401910 {'data_source': 'openai/gsm8k', 'prompt': '[{\'content\': \'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take? Let\\\'s think step by step and output the final answer after "####".\', \'role\': \'user\'}]', 'ability': 'math', 'reward_model': "{'ground_truth': '3', 'style': 'rule'}", 'extra_info': "{'answer': 'It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nSo the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\\n#### 3', 'index': 1, 'question': 'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?', 'split': 'test'}", 'uid': '121ab1d2-b03c-4ea8-abda-1729f7401910'}
[36m(main_task pid=231343)[0m 1c7872d0-d969-45f8-ba42-2acb94a6efb3 {'data_source': 'openai/gsm8k', 'prompt': '[{\'content\': \'Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make? Let\\\'s think step by step and output the final answer after "####".\', \'role\': \'user\'}]', 'ability': 'math', 'reward_model': "{'ground_truth': '70000', 'style': 'rule'}", 'extra_info': "{'answer': 'The cost of the house and repairs came out to 80,000+50,000=$<<80000+50000=130000>>130,000\\nHe increased the value of the house by 80,000*1.5=<<80000*1.5=120000>>120,000\\nSo the new value of the house is 120,000+80,000=$<<120000+80000=200000>>200,000\\nSo he made a profit of 200,000-130,000=$<<200000-130000=70000>>70,000\\n#### 70000', 'index': 2, 'question': 'Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?', 'split': 'test'}", 'uid': '1c7872d0-d969-45f8-ba42-2acb94a6efb3'}
[36m(main_task pid=231343)[0m Saving processed dataset with uid to dataset/processed_with_uid_hf
[36m(main_task pid=231343)[0m Saving processed dataset as parquet to dataset/processed_with_uid.parquet
[36m(main_task pid=231343)[0m Also saving to cache: /root/.cache/verl/rlhf/processed_with_uid
[36m(main_task pid=231343)[0m Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(main_task pid=231343)[0m WARNING:2025-09-07 20:47:34,557:Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  13%|█▎        | 165/1319 [00:00<00:06, 180.81 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  38%|███▊      | 495/1319 [00:01<00:01, 534.42 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  63%|██████▎   | 825/1319 [00:01<00:00, 818.09 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  75%|███████▌  | 990/1319 [00:01<00:00, 946.20 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8):  88%|████████▊ | 1155/1319 [00:01<00:00, 1026.61 examples/s]
[36m(main_task pid=231343)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|██████████| 1319/1319 [00:01<00:00, 745.91 examples/s] 
[36m(main_task pid=231343)[0m filter dataset len: 1319
[36m(main_task pid=231343)[0m Saving filtered dataset to dataset/filtered_with_uid_hf
[36m(main_task pid=231343)[0m Saving the dataset (0/1 shards):   0%|          | 0/1319 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 43735.57 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 42885.61 examples/s]
[36m(main_task pid=231343)[0m Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 42.37ba/s]
[36m(main_task pid=231343)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(main_task pid=231343)[0m Saving filtered dataset as parquet to dataset/filtered_with_uid.parquet
[36m(main_task pid=231343)[0m Size of train dataloader: 14, Size of val dataloader: 1
[36m(main_task pid=231343)[0m Total training steps: 140
[36m(main_task pid=231343)[0m colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(main_task pid=231343)[0m WARNING:2025-09-07 20:47:37,040:Waiting for register center actor HtasMX_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(main_task pid=231343)[0m WARNING:2025-09-07 20:47:41,568:Waiting for register center actor svQKLv_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(main_task pid=231343)[0m colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(WorkerDict pid=232143)[0m Critic overriding config {'bos_token_id': None, 'eos_token_id': 151645, 'pad_token_id': 151643}
[36m(WorkerDict pid=232143)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=232143)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=232143)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=232143)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.bias', 'score.weight']
[36m(WorkerDict pid=232143)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=232143)[0m Qwen2ForTokenClassification contains 494.03M parameters
[36m(WorkerDict pid=232143)[0m /root/miniconda3/envs/verl_new/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=232143)[0m   warnings.warn(
[36m(WorkerDict pid=232143)[0m Before critic FSDP, memory allocated (GB): 0.00, memory reserved (GB): 0.00, device memory used/total (GB): 0.28/23.57
[36m(WorkerDict pid=232143)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=232143)[0m After critic FSDP, memory allocated (GB): 1.84, memory reserved (GB): 2.96, device memory used/total (GB): 3.37/23.57
[36m(WorkerDict pid=232143)[0m Total steps: 140, num_warmup_steps: 0
[36m(WorkerDict pid=232143)[0m Critic use_remove_padding=False
[36m(WorkerDict pid=231702)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=231702)[0m   "architectures": [
[36m(WorkerDict pid=231702)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=231702)[0m   ],
[36m(WorkerDict pid=231702)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=231702)[0m   "dtype": "bfloat16",
[36m(WorkerDict pid=231702)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=231702)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=231702)[0m   "hidden_size": 896,
[36m(WorkerDict pid=231702)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=231702)[0m   "intermediate_size": 4864,
[36m(WorkerDict pid=231702)[0m   "layer_types": [
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention",
[36m(WorkerDict pid=231702)[0m     "full_attention"
[36m(WorkerDict pid=231702)[0m   ],
[36m(WorkerDict pid=231702)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=231702)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=231702)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=231702)[0m   "num_attention_heads": 14,
[36m(WorkerDict pid=231702)[0m   "num_hidden_layers": 24,
[36m(WorkerDict pid=231702)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=231702)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=231702)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=231702)[0m   "rope_scaling": null,
[36m(WorkerDict pid=231702)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=231702)[0m   "sliding_window": null,
[36m(WorkerDict pid=231702)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=231702)[0m   "transformers_version": "4.56.1",
[36m(WorkerDict pid=231702)[0m   "use_cache": true,
[36m(WorkerDict pid=231702)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=231702)[0m   "vocab_size": 151936
[36m(WorkerDict pid=231702)[0m }
[36m(WorkerDict pid=231702)[0m 
[36m(WorkerDict pid=231702)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fddd4ad2290>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fddd4ad2170>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=231702)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=231702)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=231702)[0m Qwen2ForCausalLM contains 494.03M parameters
[36m(WorkerDict pid=231702)[0m /root/miniconda3/envs/verl_new/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(WorkerDict pid=231702)[0m No module named 'vllm._version'
[36m(WorkerDict pid=231702)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=231702)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=231702)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=231702)[0m /root/miniconda3/envs/verl_new/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=231702)[0m   warnings.warn(
[36m(WorkerDict pid=231702)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=231702)[0m INFO 09-07 20:47:58 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(WorkerDict pid=231702)[0m WARNING 09-07 20:47:58 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=231702)[0m local rank 0
[36m(WorkerDict pid=231702)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=231702)[0m Total steps: 140, num_warmup_steps: 0
[36m(WorkerDict pid=231702)[0m no hf weight loader need to be updated
[36m(WorkerDict pid=231702)[0m before init cache memory allocated: 2.981812736GB, reserved: 3.074424832GB
[36m(WorkerDict pid=231702)[0m after init cache memory allocated: 14.809750016GB, reserved: 14.902362112GB
[36m(WorkerDict pid=231702)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 512, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=231702)[0m /root/miniconda3/envs/verl_new/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=231702)[0m   warnings.warn(
[36m(main_task pid=231343)[0m wandb: Currently logged in as: 300shubhamjain (300shubhamjain-stevens-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=231343)[0m wandb: Tracking run with wandb version 0.21.3
[36m(main_task pid=231343)[0m wandb: Run data is saved locally in /root/verl/examples/split_placement/wandb/run-20250907_204804-aietqrlo
[36m(main_task pid=231343)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=231343)[0m wandb: Syncing run qwen_0.5b_split_test
[36m(main_task pid=231343)[0m wandb: ⭐️ View project at https://wandb.ai/300shubhamjain-stevens-institute-of-technology/verl_qwen_2gpu
[36m(main_task pid=231343)[0m wandb: 🚀 View run at https://wandb.ai/300shubhamjain-stevens-institute-of-technology/verl_qwen_2gpu/runs/aietqrlo
Error executing job with overrides: ['algorithm.adv_estimator=gae', 'data.train_files=/root/verl/dataset/train_with_uid.parquet', 'data.val_files=/root/verl/dataset/test_with_uid.parquet', 'data.train_batch_size=512', 'data.max_prompt_length=512', 'data.max_response_length=512', 'data.filter_overlong_prompts=True', 'data.truncation=error', 'data.universal_id_key=uid', 'data.reward_fn_key=data_source', 'actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.actor.ppo_mini_batch_size=128', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8', 'actor_rollout_ref.actor.ppo_max_token_len_per_gpu=4096', 'actor_rollout_ref.actor.fsdp_config.param_offload=True', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=True', '+actor_rollout_ref.actor.loss_agg_mode=token-mean', '+actor_rollout_ref.actor.clip_ratio_low=0.2', '+actor_rollout_ref.actor.clip_ratio_high=0.2', '+actor_rollout_ref.actor.checkpoint.contents=[model,optimizer,extra]', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.max_num_batched_tokens=2048', 'actor_rollout_ref.rollout.max_num_seqs=256', '+actor_rollout_ref.rollout.max_model_len=2048', '+actor_rollout_ref.rollout.mode=sync', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8', '+actor_rollout_ref.rollout.val_kwargs.do_sample=false', '+actor_rollout_ref.rollout.val_kwargs.temperature=0', '+actor_rollout_ref.rollout.val_kwargs.top_p=1.0', '+actor_rollout_ref.rollout.val_kwargs.top_k=-1', '+actor_rollout_ref.rollout.val_kwargs.n=1', '+actor_rollout_ref.rollout.multi_turn.enable=false', '+critic.rollout_n=1', '+critic.loss_agg_mode=token-mean', '+critic.checkpoint.contents=[model,optimizer,extra]', 'critic.model.path=Qwen/Qwen2.5-0.5B-Instruct', 'critic.optim.lr=1e-5', 'critic.ppo_micro_batch_size_per_gpu=16', 'critic.ppo_max_token_len_per_gpu=16384', 'critic.model.fsdp_config.param_offload=False', 'critic.model.fsdp_config.optimizer_offload=False', 'algorithm.use_kl_in_reward=False', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=verl_qwen_2gpu', 'trainer.experiment_name=qwen_0.5b_split_test', 'trainer.n_gpus_per_node=2', 'trainer.nnodes=1', 'trainer.total_epochs=10', 'trainer.save_freq=5']
Traceback (most recent call last):
  File "/root/verl/examples/split_placement/main_ppo_split.py", line 102, in main
    ray.get(main_task.remote(config))
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/ray/_private/worker.py", line 2882, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/ray/_private/worker.py", line 968, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ConfigAttributeError): [36mray::main_task()[39m (pid=231343, ip=172.17.0.6)
  File "/root/verl/examples/split_placement/main_ppo_split.py", line 203, in main_task
    trainer.fit()
  File "/root/verl/examples/split_placement/split_monkey_patch.py", line 58, in fit
    self._load_checkpoint()
  File "/root/verl/verl/trainer/ppo/ray_trainer.py", line 932, in _load_checkpoint
    self.actor_rollout_wg.load_checkpoint(actor_path, del_local_after_load=self.config.trainer.del_local_ckpt_after_load)
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/dictconfig.py", line 359, in __getattr__
    self._format_and_raise(key=key, value=None, cause=e)
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/base.py", line 231, in _format_and_raise
    format_and_raise(
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/_utils.py", line 819, in format_and_raise
    _raise(ex, cause)
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/_utils.py", line 797, in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/dictconfig.py", line 351, in __getattr__
    return self._get_impl(
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/dictconfig.py", line 442, in _get_impl
    node = self._get_child(
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/basecontainer.py", line 73, in _get_child
    child = self._get_node(
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/dictconfig.py", line 475, in _get_node
    self._validate_get(key)
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/dictconfig.py", line 164, in _validate_get
    self._format_and_raise(
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/base.py", line 231, in _format_and_raise
    format_and_raise(
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/_utils.py", line 899, in format_and_raise
    _raise(ex, cause)
  File "/root/miniconda3/envs/verl_new/lib/python3.10/site-packages/omegaconf/_utils.py", line 797, in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
omegaconf.errors.ConfigAttributeError: Key 'del_local_ckpt_after_load' is not in struct
    full_key: trainer.del_local_ckpt_after_load
    object_type=dict

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(main_task pid=231343)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(main_task pid=231343)[0m Found checkpoint: %s /root/verl/examples/split_placement/checkpoints/verl_qwen_2gpu/qwen_0.5b_split_test/global_step_20
[36m(main_task pid=231343)[0m Load from checkpoint folder: /root/verl/examples/split_placement/checkpoints/verl_qwen_2gpu/qwen_0.5b_split_test/global_step_20
[36m(main_task pid=231343)[0m Setting global step to 20
[36m(main_task pid=231343)[0m Resuming from /root/verl/examples/split_placement/checkpoints/verl_qwen_2gpu/qwen_0.5b_split_test/global_step_20
